{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MRI Datasets\n",
    "\n",
    "For this workshop and the fMRI and dwi workshops that follow, we will be using a subset of a publicly available dataset, ds000030, from [openneuro.org](https://openneuro.org/datasets/ds000030). This dataset and all others hosted on OpenNeuro is structured according to BIDS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenNeuro\n",
    "- client-side BIDS validation\n",
    "- resumable uploads\n",
    "- running BIDS apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data\n",
    "\n",
    "#### Datalad\n",
    "\n",
    "`Datalad` installs the data - which for a dataset means that we get the \"small\" data (i.e. the text files) and the download instructions for the larger files. We can now navigate the dataset like its a file system and plan our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] realpath of PWD=/projects/mjoseph is /mnt/tigrlab/projects/mjoseph whenever os.getcwd()=/mnt/tigrlab/projects/mjoseph/tutorials/carpentry/SDC-BIDS-IntroMRI/code/04-mri-datasets. From now on will be returning os.getcwd(). Directory symlinks in the paths will be resolved \n"
     ]
    }
   ],
   "source": [
    "import datalad.api as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning http://datasets.datalad.org/openfmri/ds000030 [1 other candidates] into '/mnt/tigrlab/projects/mjoseph/tutorials/carpentry/SDC-BIDS-IntroMRI/data/ds00030' \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3809757129450fb909a56abaef9708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Cloning (counting objects)', style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08727bb1b0248cbb31c1c2d3d4eb528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Cloning (compressing objects)', max=161642, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062f3c188c4b47d3b48b803597bc6fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Cloning (receiving objects)', max=358420, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab831b61f334caa990fca6f69463581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Cloning (resolving stuff)', max=212731, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00b2293bb6649a980b9dec26547a735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Cloning (checking things out)', max=9926, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] pyenv: git-annex-remote-datalad-archives: command not found \n",
      "[INFO]  \n",
      "[INFO] The `git-annex-remote-datalad-archives' command exists in these Python versions: \n",
      "[INFO]   3.7.3/envs/carpentry_venv \n",
      "[INFO]   3.7.3/envs/tigrlab_venv \n",
      "[INFO]   carpentry_venv \n",
      "[INFO]   tigrlab_venv \n",
      "[INFO] pyenv: git-annex-remote-datalad-archives: command not found \n",
      "[INFO] The `git-annex-remote-datalad-archives' command exists in these Python versions: \n",
      "[INFO]   3.7.3/envs/carpentry_venv \n",
      "[INFO]   3.7.3/envs/tigrlab_venv \n",
      "[INFO]   carpentry_venv \n",
      "[INFO]   tigrlab_venv \n",
      "[INFO]   external special remote protocol error, unexpectedly received \"\" (unable to parse command) \n"
     ]
    }
   ],
   "source": [
    "ds = dl.install('../../data/ds000030', source='///openfmri/ds000030')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting and dropping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.get('../../data/ds000030/sub-10159')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.drop('../../data/ds000030/sub-10159')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Web Services (AWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --no-sign-request \\\n",
    "  s3://openneuro/ds000030/ds000030_R1.0.5/uncompressed/sub-10159 \\\n",
    "  data/ds000030/sub-10159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --no-sign-request \\\n",
    "  s3://openneuro/ds000030/ds000030_R1.0.5/uncompressed/sub-10159 \\\n",
    "  data/ds000030/sub-10159 \\\n",
    "  --exclude '*' \\\n",
    "  --include '*task-rest_bold*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Data\n",
    "\n",
    "Below is a tree diagram showing the folder structure of single MR session within ds000030. This was obtained by using the bash command `tree`.  \n",
    "`!tree data/ds000030`\n",
    "\n",
    "```\n",
    "ds000030\n",
    "├── CHANGES\n",
    "├── dataset_description.json\n",
    "├── derivatives\n",
    "│   └── fmriprep\n",
    "├── participants.tsv\n",
    "├── README\n",
    "├── sub-50083\n",
    "│   ├── anat\n",
    "│   │   ├── sub-50083_T1w.json\n",
    "│   │   └── sub-50083_T1w.nii.gz\n",
    "│   └── func\n",
    "│       ├── sub-50083_task-rest_bold.json\n",
    "│       └── sub-50083_task-rest_bold.nii.gz\n",
    "└── task-rest_bold.json\n",
    "```\n",
    "\n",
    "The `participants.tsv` file is meant to describe some demographic information on each participant within your study (eg. age, handedness, sex, etc.) Let's take a look at the `participants.tsv` file to see what's been included in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to load the data into Python, we'll need to import the `pandas` package. The `pandas` **dataframe** is Python's equivalent to an Excel spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `read_csv()` function. It requires us to specify the name of the file we want to import and the separator that is used to distinguish each column in our file (`\\t` since we're working with a `.tsv` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata = pd.read_csv('../../data/ds000030/participants.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a glimpse of our data, we'll use the `head()` function. By default, `head` prints the first 5 rows of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view any number of rows by specifying `n=?` as an argument within `head()`.  \n",
    "If we want to select particular rows within the dataframe, we can use the `loc[]` function and identify the rows we want based on their index label (the numbers in the left-most column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata.loc[[6, 10, 12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Select the first 5 rows of the dataframe using `loc[]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata.loc[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** How many participants do we have in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 different methods of selecting columns in a dataframe:  \n",
    "*  participant_metadata[`'<column_name>'`] (this is similar to selecting a key in a Python dictionary)  \n",
    "*  participant_metadata.`<column_name>`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see how many participants are in the study is to select the `participant_id` column and use the `count()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata['participant_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** Which diagnosis groups are part of the study?  \n",
    "*Hint: use the* `unique()` *function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata['diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to count the number of participants in each diagnosis group, we can use the `value_counts()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE:** How many males and females are in the study? How many are in each diagnosis group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata.groupby(['diagnosis', 'gender']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the participant dataframe, we noticed that there is a column called `ghost_NoGhost`. We should look at the README file that comes with the dataset to find out more about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../data/ds000030/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we're just going to work with participants that are either CONTROL or SCHZ (`diagnosis`) and have both a T1w (`T1w == 1`) and rest (`rest == 1`) scan. Also, we'll only use data without a ghosting artifact in the T1w scan (`ghost_NoGhost == 'No_ghost'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>EXERCISE:</b> Filter <code>participant_metadata</code> so that only the above conditions are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_metadata = participant_metadata[(participant_metadata.diagnosis.isin(['CONTROL', 'SCHZ'])) & \n",
    "                                            (participant_metadata.T1w == 1) & \n",
    "                                            (participant_metadata.rest == 1) & \n",
    "                                            (participant_metadata.ghost_NoGhost == 'No_ghost')]\n",
    "participant_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease the analysis and quicken the amount of time required to download the data, we're just going to use scans from 10 randomly sampled CONTROL and 10 SCHZ participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_groups = participant_metadata.groupby('diagnosis')\n",
    "filtered_participant_metadata = diagnosis_groups.apply(lambda x: x.sample(n = 10))\n",
    "filtered_participant_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_list = filtered_participant_metadata.participant_id.tolist()\n",
    "participant_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already randomly sampled 10 CONTROL and 10 SCHZ participants and placed the participant list in the `../download_list` text file. Let's download that data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # download T1w scans\n",
    "# !cat ../download_list | \\\n",
    "#   xargs -I '{}' aws s3 sync --no-sign-request \\\n",
    "#   s3://openneuro/ds000030/ds000030_R1.0.5/uncompressed/{}/anat \\\n",
    "#   ../data/ds000030/{}/anat\n",
    "\n",
    "# # download resting state fMRI scans\n",
    "# !cat ../download_list | \\\n",
    "#   xargs -I '{}' aws s3 sync --no-sign-request \\\n",
    "#   s3://openneuro/ds000030/ds000030_R1.0.5/uncompressed/{}/func \\\n",
    "#   ../data/ds000030/{}/func \\\n",
    "#   --exclude '*' \\\n",
    "#   --include '*task-rest_bold*'\n",
    "\n",
    "# # download fmriprep preprocessed anat data\n",
    "# !cat ../download_list | \\\n",
    "#   xargs -I '{}' aws s3 sync --no-sign-request \\\n",
    "#   s3://openneuro/ds000030/ds000030_R1.0.5/uncompressed/derivatives/fmriprep/{}/anat \\\n",
    "#   ../data/ds000030/derivatives/fmriprep/{}/anat\n",
    "\n",
    "# # download fmriprep preprocessed func data\n",
    "# !cat ../download_list | \\\n",
    "#   xargs -I '{}' aws s3 sync --no-sign-request \\\n",
    "#   s3://openneuro/ds000030/ds000030_R1.0.5/uncompressed/derivatives/fmriprep/{}/func \\\n",
    "#   ../data/ds000030/derivatives/fmriprep/{}/func \\\n",
    "#   --exclude '*' \\\n",
    "#   --include '*task-rest_bold*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying a BIDS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pybids](https://bids-standard.github.io/pybids/) is a Python API for querying, summarizing and manipulating the BIDS folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids.layout import BIDSLayout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = BIDSLayout('../data/ds000030')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pybids layout object lets you query your BIDS dataset according to a number of parameters by using a `get_*()` method.  \n",
    "We can get a list of the subjects we've downloaded from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_subjects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a list of all of the files, just use `get()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many arguments we can use to filter down this list. Any BIDS-defined keyword can be passed on as a constraint. In `pybids`, these keywords are known as **entities**. For a complete list of possibilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we only want the file paths of all of our resting state fMRI scans,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get(datatype='func', suffix='bold', task='rest', extensions=['.nii.gz'], return_type='file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**: Retrieve the file paths of any scan where the subject is '10292' or '50081' and the `RepetitionTime` is 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get(subject=['10292', '50081'], RepetitionTime=2, return_type='file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the first file from our list of file paths to a variable and pull the metadata from its associated JSON file using the `get_metadata()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_file = layout.get(subject=['10292', '50081'], RepetitionTime=2, return_type='file')[0]\n",
    "layout.get_metadata(fmri_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even collect the metadata for all of our fmri scans into a list and convert this into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_list = []\n",
    "all_fmri_files = layout.get(datatype='func', suffix='bold', return_type='file', extensions='.nii.gz')\n",
    "for fmri_file in all_fmri_files:\n",
    "    fmri_metadata = layout.get_metadata(fmri_file)\n",
    "    metadata_list.append(fmri_metadata)\n",
    "df = pd.DataFrame.from_records(metadata_list)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carpentry_venv",
   "language": "python",
   "name": "carpentry_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
